{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -p c:\\Users\\imava\\MCQgenerator\\env ipykernel --update-deps --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv  #take environment variables from .env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(openai_api_key = KEY, model_name = \"gpt-3.5-turbo\", temperature = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Till now we have made the setup and initialised our llm , and now we will start implementing\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "#from langchain.chat_models import ChatOpenAI  #already imported\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "#from dotenv import load_dotenv   #already imported\n",
    "import PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_JSON = {\n",
    "        \"1\": {\n",
    "             \"mcq\" : \"multiple choice question\",\n",
    "            \"options\" : {\n",
    "                    \"a\" : \"choice here\",\n",
    "                    \"b\" : \"choice here\",\n",
    "                    \"c\" : \"choice here\",\n",
    "                    \"d\" : \"choice here\",\n",
    "        `},\n",
    "        \"correct\" : \"correct answer\",\n",
    "    },\n",
    "    \"2\": {\n",
    "             \"mcq\" : \"multiple choice question\",\n",
    "            \"options\" : {\n",
    "                    \"a\" : \"choice here\",\n",
    "                    \"b\" : \"choice here\",\n",
    "                    \"c\" : \"choice here\",\n",
    "                    \"d\" : \"choice here\",\n",
    "        },\n",
    "        \"correct\" : \"correct answer\",\n",
    "    },\n",
    "    \"3\": {\n",
    "             \"mcq\" : \"multiple choice question\",\n",
    "            \"options\" : {\n",
    "                    \"a\" : \"choice here\",\n",
    "                    \"b\" : \"choice here\",\n",
    "                    \"c\" : \"choice here\",\n",
    "                    \"d\" : \"choice here\",\n",
    "        },\n",
    "        \"correct\" : \"correct answer\",\n",
    "    },\n",
    "    \"4\": {\n",
    "             \"mcq\" : \"multiple choice question\",\n",
    "            \"options\" : {\n",
    "                    \"a\" : \"choice here\",\n",
    "                    \"b\" : \"choice here\",\n",
    "                    \"c\" : \"choice here\",\n",
    "                    \"d\" : \"choice here\",\n",
    "        },\n",
    "        \"correct\" : \"correct answer\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "TEXT: {text}\n",
    "You are an expert MCQ maker. Given the above text, it is your job to \\\n",
    "create a quiz of {number} multiple questions for {subject} student in {tone} tone.\n",
    "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
    "Make sure to format your response like RESPONSE_JSON below and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables = [\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],       #here 'tone' is difficulty level\n",
    "    template = TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will chain the llm and the prompt\n",
    "quize_chain = LLMChain(llm = llm, prompt = quiz_generation_prompt, output_key = \"quiz\", verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# now we will write a prompt for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEMPLATE2 = \"\"\"\n",
    "You are an expert English Grammarian and writer Professor. Given a Multiple Choice Quiz for {subject} students. \\\n",
    "You need to evaluate the complexity of the question and give a complete analysis of the quiz. Only use at max 50 words for complexity, if the quiz is not at per with the coginitive and analytical abilities of the students, \\\n",
    "update the quiz questions which needs to be changed and change the tone such that it perfectly fits the student 's ability.\n",
    "Quiz_MCQs:\n",
    "{quiz}\n",
    "\n",
    "Check from an expert English writer of the above quiz:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a template for evaluation and this input will come from user side\n",
    "quiz_evaluation_prompt = PromptTemplate(input_variables=[\"subject\", \"quiz\"], template = TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a chain for the above prompt\n",
    "review_chain = LLMChain(llm = llm, prompt = quiz_evaluation_prompt, output_key=\"review\", verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now connecting both the chains using sequential chains\n",
    "generate_evaluate_chain = SequentialChain(chains=[quiz_chain, review_chain], input_variables=['text', \"number\", \"subject\", \"tone\", \"response_json\"], output_variables=[\"quiz\", \"review\"], verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\imava\\MCQgenerator\\env\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\imava\\MCQgenerator\\env ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "file_path = r \"C:\\Users\\imava\\MCQgenerator\\experiment\\data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to read the data\n",
    "with open(file_path, \"r\") as\n",
    "    TEXT = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the content\n",
    "print(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE the RESPONSE_Json is in the dictionary format; Serialise the Python dictionary into a JSON-formatted string\n",
    "json.dumps(RESPONSE_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER = 5\n",
    "SUBJECT = \"Generative AI\"\n",
    "TONE = \"medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating final response\n",
    "\n",
    "#https://python.langchain.com/docs/modules/model_io/llms/token_usage_tracking\n",
    "\n",
    "# How to setup Token usage Tracking in LangChain\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    response = generate_evaluate_chain(\n",
    "        {\n",
    "            'text': TEXTt,\n",
    "            \"number\" : NUMBER,\n",
    "            \"subject\" : SUBJECT, \n",
    "            \"tone\" : TONE,\n",
    "            \"response_json\" : json.dumps(RESPONSE_JSON)\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generated tokens cost\n",
    "print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "print(f\"Total Cost; {cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generated Response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the questions for the quiz\n",
    "quiz = response.get(\"quiz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the questions in our desired json format which we have formatted\n",
    "quiz = json.loads(quiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting the quiz questions segregating by \"|\"\n",
    "\n",
    "quiz_table_data = []\n",
    "for key, value in quiz.items():\n",
    "    mcq = value[\"mcq\"]\n",
    "    options = \"|\".join(\n",
    "        [\n",
    "            f\"{option}: {option_value}\"\n",
    "            for option, option_value in value[\"options\"].items()\n",
    "        ]\n",
    "    )\n",
    "    correct = value[\"correct\"]\n",
    "    quiz_table_data.append({\"MCQ\"; mcq, \"Choices\": options, \"Correct\": correct})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting it to the dataframe\n",
    "quiz = pd.DataFrame(quiz_table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz.to_csv(\"GenerativeAI.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
